# これなに？

論文読んだときのメモ．思いついたことや考えたことを適当に書き込む．

## Sequence to Sequence Learning with Neural Networks (2014)

### Abstract
DNNは難しい学習タスクで良好な性能を発揮する強力なモデルである．DNNは大量のラベル付けされた学習データセットが利用可能な場合はうまく機能するが，系列から系列への写像（変換）には使えない．本論文において，我々は系列学習のための一般的なend-to-endアプローチを提案する．このアプローチでは，系列構造で最小限の仮定を設ける．我々の手法では，1個の多層Long-Short-Term-Memory(LSTM)を使って入力系列を固定次元のベクトルに写像（変換）する．次に，もう一つの深層LSTMをこのベクトルからターゲット（目的変数）系列へデコードする．主要な結果は，WMT-14データセットによる英語からフランス語への変換であり，このLSTMによる翻訳は全テストセットで34.8 BLEUスコアを達成した．（このLSTMによるBLEUスコアは辞書に含まれない単語を課した結果である．）更に付け加えると，このLSTMは長い系列を苦にしなかった．性能比較では，フレーズベースのSMTシステムは同じデータセットで33.3 BLEUスコアに達した．LSTMに，前述のSMTシステムによる1000の仮説を順位付けした場合，BLEUスコアは36.5まで上昇する．このスコアは，直近のstate of the artに近い．また，このLSTMは，語順に厳しかったり，能動態と受動態であまり変化しなかったりするような，気の利いたフレーズやセンテンス表現も学習させた．その結果，（ターゲット含まない）全てのセンテンスで語順をひっくり返したものであってもLSTMの性能を飛躍的に上昇させた．それは，入出力のセンテンス間における短期間の依存関係が大量に導入されることにより，最適化問題をより簡単に出来たからである．


### 一言でいうと？（間違ってた場合はあとで書き直す）

系列の入力に対して，LSTMを固定次元のベクトルに変換したものをencoderとする．出力系列に対して，この固定長ベクトルからLSTMに戻したものをdecoderとした．これによって，入出力間における短期間のセンテンスの依存関係を学習でき，最適化を簡単に解けるようになり，語順を変えた表現でも高性能なモデルとなっている．

### Introduction
- DNNは強力
    - 任意に適当なステップ数で並列計算ができるから．（例　Nビットの数をたった2次元，2つの隠れ層だけを使ってソートできる）
    - NNは従来の統計モデルに関連しているのに，複雑な計算を学習できる．
    - 大規模なDNNであっても学習可能．これは，大量にラベル付けされた学習データを逆誤差伝搬で学習することでネットワークのパラメータを決定できるから．

- 従来のDNNは，入出力がうまく固定次元にエンコードされた問題にしか適用できない．これはかなりの制約である．
    - 多数の主要な系列から系列へ変換する課題では，一番よい系列表現がなされるが，系列長は未知である．
        - 例　音声認識や機械翻訳は系列長の課題，質問応答
    → （入出力？）領域が独立の問題は世の役に立つのは明らか．

- 系列問題は難しい．DNNでは，事前に系列の入出力次元が分かっていなければならないので．
    - 【手法】LSTMの構造を使えば，一般的なseq2seq問題を解ける．
        - LSTMは入力系列を読み（1回に1タイムステップ），多（大）次元のベクトル表現を得る．そして，もう一つLSTMを使いこのベクトルから出力系列を抽出する．（下図）



### モデル

### 関連研究

### 結論


### 考えたこと
- 語順変化にロバストな性能を持つということは，日本語に有利に働くのでは？
    - 実際にはどうなのか？
