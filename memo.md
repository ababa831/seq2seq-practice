# これなに？

論文読んだときのメモ．ざっくり和訳＋思いついたことや考えたことを適当に書き込む．

## [Sequence to Sequence Learning with Neural Networks (2014)](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

### Abstract
DNNは難しい学習タスクで良好な性能を発揮する強力なモデルである．DNNは大量のラベル付けされた学習データセットが利用可能な場合はうまく機能するが，系列から系列への写像（変換）には使えない．本論文において，我々は系列学習のための一般的なend-to-endアプローチを提案する．このアプローチでは，系列構造で最小限の仮定を設ける．我々の手法では，1個の多層[Long-Short-Term-Memory(LSTM)](http://www.bioinf.jku.at/publications/older/2604.pdf)を使って入力系列を固定次元のベクトルに写像（変換）する．次に，もう一つの深層LSTMをこのベクトルからターゲット（目的変数）系列へデコードする．主要な結果は，WMT-14データセットによる英語からフランス語への変換であり，このLSTMによる翻訳は全テストセットで34.8 BLEUスコアを達成した．（このLSTMによるBLEUスコアは辞書に含まれない単語を課した結果である．）更に付け加えると，このLSTMは長い系列を苦にしなかった．性能比較では，フレーズベースのSMTシステムは同じデータセットで33.3 BLEUスコアに達した．LSTMに，前述のSMTシステムによる1000の仮説を順位付けした場合，BLEUスコアは36.5まで上昇する．このスコアは，直近のstate of the artに近い．また，このLSTMは，語順に厳しかったり，能動態と受動態であまり変化しなかったりするような，気の利いたフレーズやセンテンス表現も学習させた．その結果，（ターゲット含まない）全てのセンテンスで語順をひっくり返したものであってもLSTMの性能を飛躍的に上昇させた．それは，入出力のセンテンス間における短期間の依存関係が大量に導入されることにより，最適化問題をより簡単に出来たからである．


### 一言でいうと？（間違ってた場合はあとで書き直す）

DNNによる系列長の異なる入出力関係を学習する手法を提案した．系列の入力に対して，LSTMを固定次元のベクトルに変換したものをencoderとする．出力系列に対して，この固定長ベクトルからLSTMに戻したものをdecoderとした．これによって，入出力間における短期間のセンテンスの依存関係を学習でき，最適化を簡単に解けるようになり，語順を変えた表現でも高性能なモデルとなっている．

### Introduction
- DNNは強力
    - 任意に適当なステップ数で並列計算ができるから．（例　Nビットの数をたった2次元，2つの隠れ層だけを使ってソートできる）
    - NNは従来の統計モデルに関連しているのに，複雑な計算を学習できる．
    - 大規模なDNNであっても学習可能．これは，大量にラベル付けされた学習データを逆誤差伝搬で学習することでネットワークのパラメータを決定できるから．

- 従来のDNNは，入出力がうまく固定次元にエンコードされた問題にしか適用できない．これはかなりの制約である．
    - 主要な系列から系列へ変換する課題では，一番よい系列表現がなされるが，系列長は未知である．
        - 例　音声認識や機械翻訳は系列長の課題，質問応答
    - → （入出力？）領域が独立になっている問題は世の役に立つのは明らか．

- 系列問題は難しい．DNNでは，事前に系列の入出力次元が分かっていなければならないので．
    - 【手法】LSTMの構造を使えば，一般的なseq2seq問題を解ける．
        - LSTMは入力系列を読み（1回に1タイムステップ），多（大）次元のベクトル表現を得る．そして，もう一つLSTMを使いこのベクトルから出力系列を抽出する．（下図）
        - 二つ目のLSTM（decoder側）は，入力系列によって調整された点以外は基本的にRNN言語モデルである．
        - LSTMをつかって一時的に保存された長期間の依存関係をもつデータを上手く学習できる能力は，入力とそれに対応する出力間におけるかなりのタイムラグを保持できる下図のような構造をとっているおかげ．

![seq2seq_model](https://github.com/ababa893/seq2seq-practice/blob/images/seq2seq_example.png?raw=true)

- 類似研究は多数存在する．（後でリンクを追加）

- 本論文の結果は次の通り．
    - ＜途中＞



### モデル
\* GitHubのmarkdownでは，数式がレンダリングされないので，所々表記を簡略化している．

Recurrent Neural Network (RNN)は，系列に対するフィードフォワードNNが一般化されたものである．入力系列`x_1~x_T`が与えられたとき，一般的なRNNは次の反復式によって出力系列`y_1~y_T`を計算する．

![rnn_basic_eq](https://github.com/ababa893/seq2seq-practice/blob/images/rnn_basic_equation.PNG?raw=true)

RNNは，事前に入出力間の系列の並び方がわかっている場合なら，簡単に系列から系列へと写像（変換）できる．しかし，入出力の間に複雑で非単調な関係を伴う系列長の異なる問題に対しては，どのようにしてRNNを適用すれば良いか明らかにされていない．

シンプルな戦略として，入力系列をRNNによって固定長のベクトルに写像し，続いて別のRNNによってこのベクトルを出力系列へ写像（変換）する方法がある．([Choらの手法]()) この手法では，RNNがすべての関係性を与えられているから上手く機能するのであるが，長期間の依存関係を表現するようなRNNを学習するのは難しい．しかし，LSTMは長期的で一時的な関係性をもつ問題を学習できることで知られているので，LSTMを変わりに設定すれば上手くいくかもしれない．

LSTMの目的は，条件付確率`p(y_1, ..., y_T | x_1, ..., x_T')`を推定することである，（`(x_1, ..., x_T)`は入力系列，`(y_1, ..., y_T')`は出力系列．T'は入力系列長とは異なる可能性がある．）LSTMは，その直前の隠れ状態（LSTM）から出力された固定長の入力系列`v`をもとに，条件付確率`p(y_t | v, y_1, ..., y_t-1)`を計算する．そして，vとして表現した初期の隠れ状態を使って，標準的なLSTM-LM (Language Model) による出力系列の確率`y_1, ..., y_T'`を計算する．これを定式化すると，次の通り．

![seq2seq_equation](https://github.com/ababa893/seq2seq-practice/blob/images/seq2seq_equation.PNG?raw=true)

この式において，各`p(y_t | v, y_1, ..., y_t-1)`の分布は，辞書に含まれるすべての単語をsoftmax関数で表現したものである．LSTMは[Graves]()の式を用いる．各センテンスの終端は，"\<EOS\>"が付加されることに注意されたい．この記号によって，全ての存在しうる系列長の分布を定義したモデルを構築可能にしている．これを表現した概略図は，Introductionで紹介した図の通りであり，この図ではLSTMは “A”, “B”, “C”, “\<EOS\>”の表現を計算し，この表現をもとに“W”, “X”, “Y”, “Z”, “\<EOS\>”を計算している．

実際のモデルは，次の3種類の点で上の説明とは異なっている．
- 2つの異なるLSTMを使用
    - 一つは入力系列，もう一つは出力系列．
        - 計算コストが無視できる程度にモデル数パラメータを増加でき，2つの言語（系列）のペアを同時にLSTMで学習させるのが自然であるから．
- 4層のLSTMを使用
    - 層の深いLSTMの方が，層の浅いLSTMよりも性能が良いことが分かったので．
- 入力系列の語順を逆にする
    - LSTMは，系列`a, b, c`から系列` α, β, γ`に写像（変換）する代わりに，`c, b, a`から` α, β, γ`に写像（変換）する．
        - `a`と`α`，`b`と`β`の方が近い関係にあるから．
            - これにより，上の関係性のような入出力間の”確立したコミュニケーション（通信）”をSGD（最適化）が簡単に解釈できるようになっている．

上記の点を守ることで，シンプルなモデルであっても飛躍的にLSTMの性能を上昇させることができる．

### 実験　（細かい説明は省略）
WMT’14 English to French MT タスク

#### データセット
- WMT’14 English to French データセット
    - a subset of 12M sentences consisting of 348M French words and 304M English words
- 単語をベクトル表現に変換するのが一般的である．両言語ともに固定長のベクトル表現を使う．
    - ソース言語は16万語の辞書，ターゲット言語は8万語の辞書を使用．
    - 未知語が出てきた場合は，"UNK"のトークンを与える． 

#### Decoding and Rescoring

#### ソースセンテンスの反転

#### 学習の詳細
- 4層のLSTM
    - 各層で1000（メモリ）セル
    - 1000次元の単語embedding（ソース言語16万語，ターゲット言語8万語）
    - 層を増やすごとに約10%の[perplexity](https://www.slideshare.net/hoxo_m/perplexity)(評価指標)が小さくなる．おそらく隠れ状態がより大きくなるためかと．
    - 出力層の活性化関数に，8万語のsoftmaxを用いる
    - 更なる詳細
        - LSTMの初期パラメータを`-0.08 ~ 0.08`の連続一様分布で与える
        - momentumなしのSGDを使い，学習率は`0.7`．5エポック以降は半エポックごとに学習率を半分にした．最終的に，7.5エポック回した．
        - バッチサイズは128
        - LSTMは勾配損失はあまり気にしなくていいけど，勾配爆発の可能性は考えないとだめ．勾配のノルムに強い制約を課す．ノルム値が閾値を超えたらスケーリングする．
            -  (後でスケーリング式を書く)
        - 系列の長さは，大部分は20-30程度の短さであるが，一部に100以上の長さもある．つまり，バッチの中には多数の短文とわずかな長文がランダムに選ばれる．これにより，殆どのミニバッチ計算は無駄になってしまう．この問題に対処するために，各ミニバッチではだいたい同じ長さの文章が含まれるようにした．これによって，処理速度が2倍になった．



### 関連研究

### 結論


### 考えたこと
- 語順変化にロバストな性能を持つということは，日本語のように語順の制約が少ない言語にも良い性能を発揮する？
    - 実際にはどうなのか？
- ミニバッチ学習の項目において，系列長がバラバラになると，計算がほとんど無駄になるってどういうことかいまいち理解できていない．系列長に差がありすぎると上手く学習してくれないのか？短文ばかりに過学習して，長文の学習がうまくいかないということ？
