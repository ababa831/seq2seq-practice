{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å„ç¨®Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqã®ä¸€ç•ªåŸºæœ¬ã¨ãªã‚‹ï¼Œ4å±¤ã®LSTMã«ã‚ˆã‚‹encoder-decoderãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, CuDNNLSTM, TimeDistributed\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "def get_wakati(text, eos=True, reverse=False):\n",
    "    \"\"\"\n",
    "    text: æ–‡æ›¸(str) \n",
    "    eos=Trueã®å ´åˆã¯ï¼Œend of sentenceï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®çµ‚ç«¯ï¼‰è¨˜å·ã‚’ä»˜åŠ ã™ã‚‹\n",
    "    reverse=Trueã®å ´åˆã¯ï¼Œæ–‡å­—ã®ä¸¦ã³ã‚’åè»¢ã•ã›ã‚‹ï¼ï¼ˆseq2seqã§ã¯ï¼Œå…¥åŠ›ç³»åˆ—ã¯åè»¢ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ï¼ï¼‰\n",
    "    \"\"\"\n",
    "    wakati_list = t.tokenize(text, wakati=True)\n",
    "    if reverse == True:\n",
    "        wakati_list = list(reversed(wakati_list))\n",
    "    if eos == True:\n",
    "        wakati_list.append(\"<EOS>\")\n",
    "    return \" \".join(wakati_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(df, target_colname):\n",
    "    total_word_count = Counter()\n",
    "    for text in df[target_colname]: \n",
    "        total_word_count += Counter(text.split())\n",
    "    return len(total_word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trn_feats(df, inp_colname=\"wakati_input\", out_colname=\"wakati_output\"):\n",
    "    \"\"\"\n",
    "    åˆ†ã‹ã¡æ›¸ãæ¸ˆã¿ã®æ–‡æ›¸(å…¥å‡ºåŠ›ç³»åˆ—)ã‚’ï¼Œç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ï¼\n",
    "    \n",
    "    [arguments]\n",
    "    df: å…¥å‡ºåŠ›é–¢ä¿‚ã‚’è¡¨ã™Pandas DataFrame\n",
    "    inp_colname: å…¥åŠ›ï¼ˆencoderï¼‰ç³»åˆ—ã®ã‚«ãƒ©ãƒ å\n",
    "    out_colname: å‡ºåŠ›ï¼ˆdecoderï¼‰ç³»åˆ—ã®ã‚«ãƒ©ãƒ å\n",
    "    \n",
    "    [return values]\n",
    "    token_input: æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸå…¥åŠ›ç³»åˆ—\n",
    "    onehot_output: one_hotãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸå‡ºåŠ›ç³»åˆ—\n",
    "    NNãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å±¤ã¯ãƒ‡ãƒ¼ã‚¿å†…ã®å…¨å˜èªã«é–¢ã™ã‚‹softmaxå€¤ã‚’æ±‚ã‚ã‚‹ãŸã‚ï¼Œãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯äºˆã‚onehotåŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼\n",
    "    tokenizer: æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹éš›ã®ãƒ¢ãƒ‡ãƒ«\n",
    "    params: å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’dictã¨ã—ã¦çºã‚ãŸã‚‚ã®\n",
    "    \"\"\"\n",
    "    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®èªå½™æ•°ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    num_words = words_count(df, inp_colname) + words_count(df, out_colname)\n",
    "    \n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã«å¤‰æ›ã™ã‚‹\n",
    "    # å¤‰æ›å™¨ã®å­¦ç¿’\n",
    "    tokenizer = text.Tokenizer(num_words=num_words)\n",
    "    inout_text_list = df[inp_colname].tolist() + df[out_colname].tolist()\n",
    "    tokenizer.fit_on_texts(inout_text_list)\n",
    "    # æœ€å¤§ã®ç³»åˆ—é•·ã‚’è¨ˆç®—\n",
    "    inout_text_len = [len(text) for text in inout_text_list]\n",
    "    seq_maxlen = np.max(inout_text_len)\n",
    "    # æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›\n",
    "    token_input = tokenizer.texts_to_sequences(df[inp_colname].values)\n",
    "    token_output = tokenizer.texts_to_sequences(df[out_colname].values)\n",
    "    # ç³»åˆ—é•·ã‚’æƒãˆã‚‹\n",
    "    token_input = sequence.pad_sequences(token_input, maxlen=seq_maxlen)\n",
    "    token_output = sequence.pad_sequences(token_input, maxlen=seq_maxlen)\n",
    "    # one-hotå½¢å¼ã«å¤‰æ›\n",
    "    onehot_output = np_utils.to_categorical(token_output, num_classes=num_words)\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è«¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    params = {\n",
    "        \"seq_maxlen\": seq_maxlen,\n",
    "        \"num_words\": num_words\n",
    "    }\n",
    "    \n",
    "    return token_input, onehot_output, tokenizer, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(new_params=None):\n",
    "    \"\"\"\n",
    "    æœ€ã‚‚åŸºæœ¬çš„ãª4å±¤LSTMã«ã‚ˆã‚‹ï¼Œencoder-decoderãƒ¢ãƒ‡ãƒ«\n",
    "    Ref: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "    \n",
    "    hyper_params: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"seq_maxlen\": 145,\n",
    "        \"num_words\": 1616,\n",
    "        \"vec_len\": 128,\n",
    "        \"learning_rate\": 0.002,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"loss_func\": \"categorical_crossentropy\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    }\n",
    "    if new_params != None:\n",
    "        params.update(new_params)\n",
    "        \n",
    "    inp = Input(shape=(params[\"seq_maxlen\"], ))\n",
    "    # end-to-endå­¦ç¿’ã—ã‚„ã™ã„ãƒ™ã‚¯ãƒˆãƒ«ã«å†™åƒã™ã‚‹embeddingå±¤ï¼Œãã®å¿ƒã¯ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã«è¿‘ã„\n",
    "    emb = Embedding(params[\"num_words\"], params[\"vec_len\"])(inp) \n",
    "    dout= SpatialDropout1D(params[\"dropout_rate\"])(emb)\n",
    "    # è«–æ–‡åŒæ§˜ã«ï¼Œ4å±¤LSTMã‚’æŒ‡å®šï¼return_sequences=Trueã§æ™‚ç³»åˆ—åˆ†ã®LSTMæ¼”ç®—ã‚’å®Ÿè¡Œã™ã‚‹ï¼\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(dout)\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(lstmed)\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(lstmed)\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(lstmed)\n",
    "    out = TimeDistributed(Dense(params[\"num_words\"], activation=\"softmax\"))(lstmed)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss=params[\"loss_func\"], \n",
    "                  optimizer=Adam(lr=params[\"learning_rate\"]),\n",
    "                  metrics=params[\"metrics\"]) # BLEUã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ãŸã„ãŒï¼ŒKerasã§ã¯é›£ã—ã„ã®ã§å–ã‚Šæ•¢ãˆãšæ­£è§£ç‡ã§è¨­å®š\n",
    "    model.summary() # ãƒ¢ãƒ‡ãƒ«ã®æ§‹æˆã‚’è¡¨ç¤º\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„</td>\n",
       "      <td>ã‚ã‚ŠãŒã¨â™¡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ”¥çŒ«æ¾ğŸ”¥</td>\n",
       "      <td>çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction\n",
       "0          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect\n",
       "1       ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„           ã‚ã‚ŠãŒã¨â™¡\n",
       "2  ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼         neglect\n",
       "3          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect\n",
       "4              ğŸ”¥çŒ«æ¾ğŸ”¥  çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"seq2seq_sample.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "      <th>wakati_input</th>\n",
       "      <th>wakati_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "      <td>ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ &lt;EOS&gt;</td>\n",
       "      <td>neglect &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„</td>\n",
       "      <td>ã‚ã‚ŠãŒã¨â™¡</td>\n",
       "      <td>ã„ã„ ã„ã„ ã‚ã„ ã‹ ã¾ã™ ã­ã“ &lt;EOS&gt;</td>\n",
       "      <td>ã‚ã‚ŠãŒã¨ â™¡ &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "      <td>ï¼ ã¨ ãƒ¼ ã‚… ã ã¹ã‚Š ã¹ã‚Š ï¼ ã¡ã‚ƒã‚“ ï¼ ã®ã‚‰ &lt;EOS&gt;</td>\n",
       "      <td>neglect &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "      <td>ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ &lt;EOS&gt;</td>\n",
       "      <td>neglect &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ”¥çŒ«æ¾ğŸ”¥</td>\n",
       "      <td>çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­</td>\n",
       "      <td>ğŸ”¥ æ¾ çŒ« ğŸ”¥ &lt;EOS&gt;</td>\n",
       "      <td>çŒ« æ¾ ã•ã‚“ ãŒ ç‡ƒã‚„ã• ã‚Œ ã¦ ã„ ã¾ã™ ã­ &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction                      wakati_input  \\\n",
       "0          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect                ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ <EOS>   \n",
       "1       ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„           ã‚ã‚ŠãŒã¨â™¡            ã„ã„ ã„ã„ ã‚ã„ ã‹ ã¾ã™ ã­ã“ <EOS>   \n",
       "2  ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼         neglect  ï¼ ã¨ ãƒ¼ ã‚… ã ã¹ã‚Š ã¹ã‚Š ï¼ ã¡ã‚ƒã‚“ ï¼ ã®ã‚‰ <EOS>   \n",
       "3          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect                ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ <EOS>   \n",
       "4              ğŸ”¥çŒ«æ¾ğŸ”¥  çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­                     ğŸ”¥ æ¾ çŒ« ğŸ”¥ <EOS>   \n",
       "\n",
       "                   wakati_output  \n",
       "0                  neglect <EOS>  \n",
       "1                   ã‚ã‚ŠãŒã¨ â™¡ <EOS>  \n",
       "2                  neglect <EOS>  \n",
       "3                  neglect <EOS>  \n",
       "4  çŒ« æ¾ ã•ã‚“ ãŒ ç‡ƒã‚„ã• ã‚Œ ã¦ ã„ ã¾ã™ ã­ <EOS>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"wakati_input\"] = train_df[\"__text\"].apply(lambda x: get_wakati(x, reverse=True))\n",
    "train_df[\"wakati_output\"] = train_df[\"reaction\"].apply(lambda x: get_wakati(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®èªå½™æ•°ã‚’è¨ˆç®—ã™ã‚‹\n",
    "nword_in = words_count(train_df, \"wakati_input\")\n",
    "nword_out = words_count(train_df, \"wakati_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã«å¤‰æ›ã™ã‚‹\n",
    "# 1. å¤‰æ›å™¨ã®å­¦ç¿’\n",
    "num_words = nword_in + nword_out\n",
    "tokenizer = text.Tokenizer(num_words=num_words)\n",
    "inout_text_list = train_df[\"wakati_input\"].tolist() + train_df[\"wakati_output\"].tolist()\n",
    "tokenizer.fit_on_texts(inout_text_list)\n",
    "# 2. æœ€å¤§ã®ç³»åˆ—é•·ã‚’è¨ˆç®—\n",
    "inout_text_len = [len(text) for text in inout_text_list]\n",
    "seq_maxlen = np.max(inout_text_len)\n",
    "# 3. æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›\n",
    "token_input = tokenizer.texts_to_sequences(train_df[\"wakati_input\"].values)\n",
    "token_output = tokenizer.texts_to_sequences(train_df[\"wakati_output\"].values)\n",
    "# ç³»åˆ—é•·ã‚’æƒãˆã‚‹\n",
    "token_input = sequence.pad_sequences(token_input, maxlen=seq_maxlen)\n",
    "token_output = sequence.pad_sequences(token_output, maxlen=seq_maxlen)\n",
    "# ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯softmaxã§æ¨å®šã™ã‚‹ã®ã§ï¼Œäºˆã‚one-hotå½¢å¼ã«å¤‰æ›ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹ï¼\n",
    "onehot_output = np_utils.to_categorical(token_output, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ãŸç‰¹å¾´é‡ã®ä¾‹\n",
    "token_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0ã¯æ–‡å­—åˆ—ãªã—ï¼Œ1ã¯EOSï¼ãã‚Œä»¥å¤–ã¯ï¼Œå˜èªãŒæ•°å€¤ã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹\n",
    "é€†å¤‰æ›ã™ã‚‹å ´åˆã¯ï¼Œ1ä»¥ä¸Šã®è¦ç´ ã‚’æŠ½å‡ºã—ã¦ï¼Œ\n",
    "\n",
    "```\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "```\n",
    "\n",
    "ã‚’è¡Œãˆã°è‰¯ã„ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«æˆ»ã™æ–¹æ³•\n",
    "np.argmax(onehot_output[0,143,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "val_ratio = 0.1\n",
    "X_train, X_val, y_train, y_val = train_test_split(token_input, onehot_output, random_state=seed, test_size=val_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 145)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 145, 128)          206848    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 145, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 145, 1616)         208464    \n",
      "=================================================================\n",
      "Total params: 943,696\n",
      "Trainable params: 943,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3878 samples, validate on 431 samples\n",
      "Epoch 1/8\n",
      "3878/3878 [==============================] - 48s 12ms/step - loss: 2.0744 - acc: 0.9449 - val_loss: 0.2286 - val_acc: 0.9757\n",
      "Epoch 2/8\n",
      "3878/3878 [==============================] - 44s 11ms/step - loss: 0.1829 - acc: 0.9773 - val_loss: 0.1825 - val_acc: 0.9757\n",
      "Epoch 3/8\n",
      "3878/3878 [==============================] - 42s 11ms/step - loss: 0.1688 - acc: 0.9773 - val_loss: 0.1817 - val_acc: 0.9757\n",
      "Epoch 4/8\n",
      "3878/3878 [==============================] - 48s 12ms/step - loss: 0.1683 - acc: 0.9773 - val_loss: 0.1809 - val_acc: 0.9757\n",
      "Epoch 5/8\n",
      "3878/3878 [==============================] - 50s 13ms/step - loss: 0.1681 - acc: 0.9773 - val_loss: 0.1804 - val_acc: 0.9757\n",
      "Epoch 6/8\n",
      "3878/3878 [==============================] - 50s 13ms/step - loss: 0.1679 - acc: 0.9773 - val_loss: 0.1803 - val_acc: 0.9757\n",
      "Epoch 7/8\n",
      "3878/3878 [==============================] - 43s 11ms/step - loss: 0.1678 - acc: 0.9773 - val_loss: 0.1807 - val_acc: 0.9757\n",
      "Epoch 8/8\n",
      "3878/3878 [==============================] - 45s 12ms/step - loss: 0.1677 - acc: 0.9773 - val_loss: 0.1802 - val_acc: 0.9757\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq_model()\n",
    "# batch. epochsã¯è«–æ–‡ã‚’å‚è€ƒã«è¨­å®šï¼æœ€é©åŒ–ã¯ã—ã¦ã„ãªã„\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    # class_weight={0:0.01, 1:.99}, \n",
    "                    shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨å®šå¯¾ç§°ã®å…¥åŠ›ç³»åˆ—ã‚’ç”¨æ„\n",
    "example_testtext = \"ğŸ”¥çŒ«æ¾ğŸ”¥\" \n",
    "wakati_testtext= get_wakati(example_testtext, reverse=True)\n",
    "# æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã€€tokenizerã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ãŸã¨ãã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "token_testinput = tokenizer.texts_to_sequences(np.array([wakati_testtext]))\n",
    "# ç³»åˆ—é•·ã‚’æƒãˆã‚‹\n",
    "token_testinput = sequence.pad_sequences(token_testinput, maxlen=seq_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 326ms/step\n"
     ]
    }
   ],
   "source": [
    "onehot_test_output = model.predict(token_testinput, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨å®šã—ãŸone-hotãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "token_testoutput = [np.argmax(seq_val) for seq_val in onehot_test_output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_testoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨å®šã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã™ã¹ã¦0ã«ãªã£ã¦ã—ã¾ã†ï¼è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ï¼š\n",
    "\n",
    "- pad_sequenceã§0ã§ç©´åŸ‹ã‚ã—ã¦ã—ã¾ã£ãŸã“ã¨ã«ã‚ˆã‚Šï¼Œå…¨ã¦ã®å€¤ãŒ0ã«éå­¦ç¿’ã—ã¦ã—ã¾ã£ãŸï¼Ÿ\n",
    "- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã™ãã‚‹ï¼Ÿ\n",
    "\n",
    "ã—ã‹ã—ï¼Œpad_sequenceã§ç©´åŸ‹ã‚ã—ãªã‘ã‚Œã°ï¼ŒKerasã®å­¦ç¿’ã¯ä¸å¯èƒ½ï¼Ÿ\n",
    "\n",
    "è§£æ±ºæ¡ˆï¼š\n",
    "- è«–æ–‡ã®ã‚ˆã†ã«ï¼Œç³»åˆ—é•·ã«ã‚ˆã£ã¦ãƒãƒƒãƒã‚’åˆ†ã‘ã‚‹ï¼Ÿ\n",
    "- ãƒ‡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã›ã°0ã«éå­¦ç¿’ã—ãªããªã‚‹ï¼Ÿ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
