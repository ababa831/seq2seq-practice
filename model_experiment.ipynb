{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å„ç¨®Seq2Seqãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqã®ä¸€ç•ªåŸºæœ¬ã¨ãªã‚‹ï¼ŒLSTMã«ã‚ˆã‚‹encoder-decoderãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, CuDNNLSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "def get_wakati(text, reverse=False, offset=False):\n",
    "    \"\"\"\n",
    "    text: æ–‡æ›¸(str) \n",
    "    reverse=Trueã®å ´åˆã¯ï¼Œæ–‡å­—ã®ä¸¦ã³ã‚’åè»¢ã•ã›ã‚‹ï¼ï¼ˆseq2seqã§ã¯ï¼Œå…¥åŠ›ç³»åˆ—ã¯åè»¢ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ï¼ï¼‰\n",
    "    offset=Trueã®å ´åˆã¯ï¼Œã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ–‡\n",
    "    \"\"\"\n",
    "    wakati_list = t.tokenize(text, wakati=True)\n",
    "    if reverse == True:\n",
    "        wakati_list = list(reversed(wakati_list))\n",
    "    return \" \".join(wakati_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(df, target_cols):\n",
    "    total_word_count = Counter()\n",
    "    for col in target_cols:\n",
    "        for text in df[col]: \n",
    "            total_word_count += Counter(text.split())\n",
    "    #print(total_word_count.keys())\n",
    "    return len(total_word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charlen_count(df, target_colname):\n",
    "    df[\"tmp_wlist\"] = df[target_colname].str.split()\n",
    "    df[target_colname+\"_charlen\"] = df[\"tmp_wlist\"].apply(lambda x: len(x))\n",
    "    df = df.drop([\"tmp_wlist\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(new_params=None):\n",
    "    \"\"\"\n",
    "    æœ€ã‚‚åŸºæœ¬çš„ãª4å±¤LSTMã«ã‚ˆã‚‹ï¼Œencoder-decoderãƒ¢ãƒ‡ãƒ«\n",
    "    Ref: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "    \n",
    "    hyper_params: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"seq_maxlen\": 139,\n",
    "        \"num_words\": 1592,\n",
    "        \"vec_len\": 128,\n",
    "        \"learning_rate\": 0.002,\n",
    "        #\"dropout_rate\": 0.5,\n",
    "        \"loss_func\": \"categorical_crossentropy\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    }\n",
    "    if new_params != None:\n",
    "        params.update(new_params)\n",
    "        \n",
    "    enc_inp = Input(shape=(params[\"seq_maxlen\"], params[\"num_words\"]))\n",
    "    # decoderåˆæœŸåŒ–ç”¨ã®å†…éƒ¨çŠ¶æ…‹ã‚’å¾—ã‚‹\n",
    "    enc_out, state_h, state_c = CuDNNLSTM(params[\"vec_len\"], return_sequences=True, return_state=True)(enc_inp)\n",
    "    enc_states = [state_h, state_c]\n",
    "    \n",
    "    dec_out = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(enc_out, initial_state=enc_states)\n",
    "    seq_out = Dense(params[\"num_words\"], activation=\"softmax\")(dec_out)\n",
    "    \n",
    "    model = Model(inputs=enc_inp, outputs=seq_out)\n",
    "    model.compile(loss=params[\"loss_func\"], \n",
    "                  optimizer=Adam(lr=params[\"learning_rate\"]),\n",
    "                  metrics=params[\"metrics\"]) # BLEUã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ãŸã„ãŒï¼ŒKerasã§ã¯é›£ã—ã„ã®ã§å–ã‚Šæ•¢ãˆãšæ­£è§£ç‡ã§è¨­å®š\n",
    "    model.summary() # ãƒ¢ãƒ‡ãƒ«ã®æ§‹æˆã‚’è¡¨ç¤º\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„</td>\n",
       "      <td>ã‚ã‚ŠãŒã¨â™¡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ”¥çŒ«æ¾ğŸ”¥</td>\n",
       "      <td>çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction\n",
       "0          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect\n",
       "1       ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„           ã‚ã‚ŠãŒã¨â™¡\n",
       "2  ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼         neglect\n",
       "3          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect\n",
       "4              ğŸ”¥çŒ«æ¾ğŸ”¥  çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã¯å„è‡ªç”¨æ„ã™ã‚‹ï¼\n",
    "train_df = pd.read_csv(\"seq2seq_sample.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "      <th>wakati_enc_in</th>\n",
       "      <th>wakati_dec_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "      <td>ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„</td>\n",
       "      <td>ã‚ã‚ŠãŒã¨â™¡</td>\n",
       "      <td>ã„ã„ ã„ã„ ã‚ã„ ã‹ ã¾ã™ ã­ã“</td>\n",
       "      <td>ã‚ã‚ŠãŒã¨ â™¡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "      <td>ï¼ ã¨ ãƒ¼ ã‚… ã ã¹ã‚Š ã¹ã‚Š ï¼ ã¡ã‚ƒã‚“ ï¼ ã®ã‚‰</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼</td>\n",
       "      <td>neglect</td>\n",
       "      <td>ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ğŸ”¥çŒ«æ¾ğŸ”¥</td>\n",
       "      <td>çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­</td>\n",
       "      <td>ğŸ”¥ æ¾ çŒ« ğŸ”¥</td>\n",
       "      <td>çŒ« æ¾ ã•ã‚“ ãŒ ç‡ƒã‚„ã• ã‚Œ ã¦ ã„ ã¾ã™ ã­</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction               wakati_enc_in  \\\n",
       "0          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect                ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ   \n",
       "1       ã­ã“ã¾ã™ã‹ã‚ã„ã„ã„ã„ã„           ã‚ã‚ŠãŒã¨â™¡            ã„ã„ ã„ã„ ã‚ã„ ã‹ ã¾ã™ ã­ã“   \n",
       "2  ã®ã‚‰ï¼ã¡ã‚ƒã‚“ï¼ã¹ã‚Šã¹ã‚Šãã‚…ãƒ¼ã¨ï¼         neglect  ï¼ ã¨ ãƒ¼ ã‚… ã ã¹ã‚Š ã¹ã‚Š ï¼ ã¡ã‚ƒã‚“ ï¼ ã®ã‚‰   \n",
       "3          ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼         neglect                ï¼ ã£ã¨ ãã‚ƒãƒ¼ ã¯ ãŠ   \n",
       "4              ğŸ”¥çŒ«æ¾ğŸ”¥  çŒ«æ¾ã•ã‚“ãŒç‡ƒã‚„ã•ã‚Œã¦ã„ã¾ã™ã­                     ğŸ”¥ æ¾ çŒ« ğŸ”¥   \n",
       "\n",
       "            wakati_dec_out  \n",
       "0                  neglect  \n",
       "1                   ã‚ã‚ŠãŒã¨ â™¡  \n",
       "2                  neglect  \n",
       "3                  neglect  \n",
       "4  çŒ« æ¾ ã•ã‚“ ãŒ ç‡ƒã‚„ã• ã‚Œ ã¦ ã„ ã¾ã™ ã­  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"wakati_enc_in\"] = train_df[\"__text\"].apply(lambda x: get_wakati(x, reverse=True))\n",
    "train_df[\"wakati_dec_out\"] = train_df[\"reaction\"].apply(lambda x: get_wakati(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®èªå½™æ•°ã‚’è¨ˆç®—ã™ã‚‹\n",
    "target_cols = [\"wakati_enc_in\", \"wakati_dec_out\"]\n",
    "num_words = words_count(train_df, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_cols:\n",
    "    train_df = charlen_count(train_df, target_colname=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç³»åˆ—é•·é †ã«ä¸¦ã¹æ›¿ãˆã‚‹ï¼ŒãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã§ç³»åˆ—é•·ã®å·®ã‚’å°ã•ãã™ã‚‹ãŸã‚ï¼\n",
    "train_df = train_df.sort_values(by=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã«å¤‰æ›ã™ã‚‹\n",
    "# 1. å¤‰æ›å™¨ã®å­¦ç¿’\n",
    "tokenizer = text.Tokenizer(num_words=num_words)\n",
    "inout_text_list = []\n",
    "for col in target_cols:\n",
    "    inout_text_list += train_df[col].tolist()\n",
    "tokenizer.fit_on_texts(inout_text_list)\n",
    "# 2. æœ€å¤§ã®ç³»åˆ—é•·ã‚’è¨ˆç®—\n",
    "inout_text_len = [len(text) for text in inout_text_list]\n",
    "seq_maxlen = np.max(inout_text_len)\n",
    "# 3. æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›\n",
    "token_enc_in = tokenizer.texts_to_sequences(train_df[\"wakati_enc_in\"].values)\n",
    "token_dec_out = tokenizer.texts_to_sequences(train_df[\"wakati_dec_out\"].values)\n",
    "# 4. ç³»åˆ—é•·ã‚’æƒãˆã‚‹\n",
    "token_enc_in = sequence.pad_sequences(token_enc_in, maxlen=seq_maxlen)\n",
    "token_dec_out = sequence.pad_sequences(token_dec_out, maxlen=seq_maxlen)\n",
    "# 5. one-hotå½¢å¼ã«å¤‰æ›\n",
    "onehot_enc_in = np_utils.to_categorical(token_enc_in, num_classes=num_words) \n",
    "onehot_dec_out = np_utils.to_categorical(token_dec_out, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ãŸç‰¹å¾´é‡ã®ä¾‹\n",
    "token_dec_out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0ã¯æ–‡å­—åˆ—ãªã—ï¼Œ1ã¯EOSï¼ãã‚Œä»¥å¤–ã¯ï¼Œå˜èªãŒæ•°å€¤ã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹\n",
    "é€†å¤‰æ›ã™ã‚‹å ´åˆã¯ï¼Œ1ä»¥ä¸Šã®è¦ç´ ã‚’æŠ½å‡ºã—ã¦ï¼Œ\n",
    "\n",
    "```\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "```\n",
    "\n",
    "ã§é€†å¤‰æ›ã®è¾æ›¸ã‚’ä½œæˆã—ã¦ï¼Œç³»åˆ—ã‚’æ±‚ã‚ã‚Œã°ã‚ˆã„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_dec_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«æˆ»ã™æ–¹æ³•\n",
    "np.argmax(onehot_dec_out[0,138,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 139, 1590)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 139, 128), ( 880640      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 139, 128)     132096      cu_dnnlstm_1[0][0]               \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 139, 1590)    205110      cu_dnnlstm_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,217,846\n",
      "Trainable params: 1,217,846\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3878 samples, validate on 431 samples\n",
      "Epoch 1/90\n",
      "3878/3878 [==============================] - 29s 7ms/step - loss: 2.1796 - acc: 0.9526 - val_loss: 0.8353 - val_acc: 0.9361\n",
      "Epoch 2/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0970 - acc: 0.9886 - val_loss: 0.8993 - val_acc: 0.9361\n",
      "Epoch 3/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0892 - acc: 0.9886 - val_loss: 0.8836 - val_acc: 0.9361\n",
      "Epoch 4/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0885 - acc: 0.9886 - val_loss: 0.8696 - val_acc: 0.9361\n",
      "Epoch 5/90\n",
      "3878/3878 [==============================] - 22s 6ms/step - loss: 0.0889 - acc: 0.9886 - val_loss: 0.8630 - val_acc: 0.9361\n",
      "Epoch 6/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0877 - acc: 0.9886 - val_loss: 0.8513 - val_acc: 0.9361\n",
      "Epoch 7/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0876 - acc: 0.9886 - val_loss: 0.8358 - val_acc: 0.9361\n",
      "Epoch 8/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0877 - acc: 0.9886 - val_loss: 0.8488 - val_acc: 0.9361\n",
      "Epoch 9/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0865 - acc: 0.9886 - val_loss: 0.8326 - val_acc: 0.9361\n",
      "Epoch 10/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0877 - acc: 0.9886 - val_loss: 0.8232 - val_acc: 0.9361\n",
      "Epoch 11/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0875 - acc: 0.9886 - val_loss: 0.8244 - val_acc: 0.9361\n",
      "Epoch 12/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0853 - acc: 0.9886 - val_loss: 0.8178 - val_acc: 0.9361\n",
      "Epoch 13/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0848 - acc: 0.9886 - val_loss: 0.8134 - val_acc: 0.9361\n",
      "Epoch 14/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0851 - acc: 0.9886 - val_loss: 0.8119 - val_acc: 0.9361\n",
      "Epoch 15/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0837 - acc: 0.9886 - val_loss: 0.8181 - val_acc: 0.9361\n",
      "Epoch 16/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0819 - acc: 0.9886 - val_loss: 0.8066 - val_acc: 0.9361\n",
      "Epoch 17/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0820 - acc: 0.9886 - val_loss: 0.8084 - val_acc: 0.9361\n",
      "Epoch 18/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0810 - acc: 0.9886 - val_loss: 0.8105 - val_acc: 0.9361\n",
      "Epoch 19/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0807 - acc: 0.9886 - val_loss: 0.8096 - val_acc: 0.9361\n",
      "Epoch 20/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0811 - acc: 0.9886 - val_loss: 0.8050 - val_acc: 0.9361\n",
      "Epoch 21/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0796 - acc: 0.9886 - val_loss: 0.8081 - val_acc: 0.9361\n",
      "Epoch 22/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0792 - acc: 0.9886 - val_loss: 0.8120 - val_acc: 0.9361\n",
      "Epoch 23/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0789 - acc: 0.9886 - val_loss: 0.8202 - val_acc: 0.9361\n",
      "Epoch 24/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0790 - acc: 0.9886 - val_loss: 0.8060 - val_acc: 0.9361\n",
      "Epoch 25/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0767 - acc: 0.9886 - val_loss: 0.8042 - val_acc: 0.9361\n",
      "Epoch 26/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0770 - acc: 0.9886 - val_loss: 0.7940 - val_acc: 0.9361\n",
      "Epoch 27/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0783 - acc: 0.9886 - val_loss: 0.8118 - val_acc: 0.9361\n",
      "Epoch 28/90\n",
      "3878/3878 [==============================] - 22s 6ms/step - loss: 0.0783 - acc: 0.9886 - val_loss: 0.8074 - val_acc: 0.9361\n",
      "Epoch 29/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0758 - acc: 0.9886 - val_loss: 0.7940 - val_acc: 0.9361\n",
      "Epoch 30/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0742 - acc: 0.9886 - val_loss: 0.7926 - val_acc: 0.9361\n",
      "Epoch 31/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0745 - acc: 0.9886 - val_loss: 0.8029 - val_acc: 0.9361\n",
      "Epoch 32/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0725 - acc: 0.9886 - val_loss: 0.7998 - val_acc: 0.9361\n",
      "Epoch 33/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0710 - acc: 0.9886 - val_loss: 0.7874 - val_acc: 0.9361\n",
      "Epoch 34/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0688 - acc: 0.9886 - val_loss: 0.7984 - val_acc: 0.9361\n",
      "Epoch 35/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0672 - acc: 0.9886 - val_loss: 0.8135 - val_acc: 0.9361\n",
      "Epoch 36/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0661 - acc: 0.9886 - val_loss: 0.7821 - val_acc: 0.9361\n",
      "Epoch 37/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0653 - acc: 0.9886 - val_loss: 0.7924 - val_acc: 0.9361\n",
      "Epoch 38/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0631 - acc: 0.9886 - val_loss: 0.7889 - val_acc: 0.9361\n",
      "Epoch 39/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0608 - acc: 0.9886 - val_loss: 0.7776 - val_acc: 0.9361\n",
      "Epoch 40/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0600 - acc: 0.9886 - val_loss: 0.7833 - val_acc: 0.9361\n",
      "Epoch 41/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0600 - acc: 0.9886 - val_loss: 0.7845 - val_acc: 0.9361\n",
      "Epoch 42/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0592 - acc: 0.9886 - val_loss: 0.7912 - val_acc: 0.9361\n",
      "Epoch 43/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0577 - acc: 0.9886 - val_loss: 0.7691 - val_acc: 0.9361\n",
      "Epoch 44/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0562 - acc: 0.9886 - val_loss: 0.7743 - val_acc: 0.9361\n",
      "Epoch 45/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0549 - acc: 0.9886 - val_loss: 0.7768 - val_acc: 0.9361\n",
      "Epoch 46/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0536 - acc: 0.9886 - val_loss: 0.7619 - val_acc: 0.9361\n",
      "Epoch 47/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0530 - acc: 0.9886 - val_loss: 0.7806 - val_acc: 0.9361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0541 - acc: 0.9886 - val_loss: 0.7663 - val_acc: 0.9361\n",
      "Epoch 49/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0516 - acc: 0.9886 - val_loss: 0.7837 - val_acc: 0.9361\n",
      "Epoch 50/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0513 - acc: 0.9886 - val_loss: 0.7657 - val_acc: 0.9361\n",
      "Epoch 51/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0517 - acc: 0.9887 - val_loss: 0.7649 - val_acc: 0.9361\n",
      "Epoch 52/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0516 - acc: 0.9886 - val_loss: 0.7472 - val_acc: 0.9361\n",
      "Epoch 53/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0503 - acc: 0.9886 - val_loss: 0.7702 - val_acc: 0.9361\n",
      "Epoch 54/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0505 - acc: 0.9886 - val_loss: 0.7483 - val_acc: 0.9361\n",
      "Epoch 55/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0489 - acc: 0.9886 - val_loss: 0.7705 - val_acc: 0.9367\n",
      "Epoch 56/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0479 - acc: 0.9894 - val_loss: 0.7741 - val_acc: 0.9361\n",
      "Epoch 57/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0763 - acc: 0.9761 - val_loss: 0.8507 - val_acc: 0.9361\n",
      "Epoch 58/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.1033 - acc: 0.9886 - val_loss: 0.7896 - val_acc: 0.9361\n",
      "Epoch 59/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0861 - acc: 0.9886 - val_loss: 0.8118 - val_acc: 0.9361\n",
      "Epoch 60/90\n",
      "3878/3878 [==============================] - 23s 6ms/step - loss: 0.0741 - acc: 0.9886 - val_loss: 0.8063 - val_acc: 0.9361\n",
      "Epoch 61/90\n",
      "3878/3878 [==============================] - 31s 8ms/step - loss: 0.0672 - acc: 0.9886 - val_loss: 0.7930 - val_acc: 0.9361\n",
      "Epoch 62/90\n",
      "3878/3878 [==============================] - 23s 6ms/step - loss: 0.0546 - acc: 0.9886 - val_loss: 0.7709 - val_acc: 0.9361\n",
      "Epoch 63/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0528 - acc: 0.9886 - val_loss: 0.7790 - val_acc: 0.9361\n",
      "Epoch 64/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0519 - acc: 0.9886 - val_loss: 0.7708 - val_acc: 0.9361\n",
      "Epoch 65/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0515 - acc: 0.9886 - val_loss: 0.7696 - val_acc: 0.9361\n",
      "Epoch 66/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0513 - acc: 0.9886 - val_loss: 0.7648 - val_acc: 0.9361\n",
      "Epoch 67/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0509 - acc: 0.9894 - val_loss: 0.7755 - val_acc: 0.9361\n",
      "Epoch 68/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0494 - acc: 0.9886 - val_loss: 0.7760 - val_acc: 0.9361\n",
      "Epoch 69/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0486 - acc: 0.9887 - val_loss: 0.7820 - val_acc: 0.9363\n",
      "Epoch 70/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0476 - acc: 0.9905 - val_loss: 0.7629 - val_acc: 0.9368\n",
      "Epoch 71/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0473 - acc: 0.9916 - val_loss: 0.7428 - val_acc: 0.9368\n",
      "Epoch 72/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0463 - acc: 0.9897 - val_loss: 0.7504 - val_acc: 0.9368\n",
      "Epoch 73/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0450 - acc: 0.9914 - val_loss: 0.7507 - val_acc: 0.9368\n",
      "Epoch 74/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0455 - acc: 0.9909 - val_loss: 0.7366 - val_acc: 0.9368\n",
      "Epoch 75/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0436 - acc: 0.9915 - val_loss: 0.7706 - val_acc: 0.9368\n",
      "Epoch 76/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0441 - acc: 0.9914 - val_loss: 0.7512 - val_acc: 0.9368\n",
      "Epoch 77/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0448 - acc: 0.9915 - val_loss: 0.7653 - val_acc: 0.9368\n",
      "Epoch 78/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0439 - acc: 0.9914 - val_loss: 0.7416 - val_acc: 0.9369\n",
      "Epoch 79/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0425 - acc: 0.9916 - val_loss: 0.7558 - val_acc: 0.9368\n",
      "Epoch 80/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0430 - acc: 0.9918 - val_loss: 0.7333 - val_acc: 0.9368\n",
      "Epoch 81/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0424 - acc: 0.9920 - val_loss: 0.7475 - val_acc: 0.9368\n",
      "Epoch 82/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0425 - acc: 0.9917 - val_loss: 0.7489 - val_acc: 0.9368\n",
      "Epoch 83/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0412 - acc: 0.9924 - val_loss: 0.7375 - val_acc: 0.9368\n",
      "Epoch 84/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0414 - acc: 0.9916 - val_loss: 0.7117 - val_acc: 0.9361\n",
      "Epoch 85/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0401 - acc: 0.9915 - val_loss: 0.7252 - val_acc: 0.9368\n",
      "Epoch 86/90\n",
      "3878/3878 [==============================] - 22s 6ms/step - loss: 0.0390 - acc: 0.9927 - val_loss: 0.7481 - val_acc: 0.9368\n",
      "Epoch 87/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0384 - acc: 0.9925 - val_loss: 0.7517 - val_acc: 0.9368\n",
      "Epoch 88/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0399 - acc: 0.9923 - val_loss: 0.7209 - val_acc: 0.9368\n",
      "Epoch 89/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0370 - acc: 0.9928 - val_loss: 0.7063 - val_acc: 0.9368\n",
      "Epoch 90/90\n",
      "1408/3878 [=========>....................] - ETA: 11s - loss: 0.0438 - acc: 0.9912"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"seq_maxlen\": seq_maxlen,\n",
    "    \"num_words\": num_words,\n",
    "    \"vec_len\": 128,\n",
    "    \"learning_rate\": 0.002,\n",
    "    #\"dropout_rate\": 0.5,\n",
    "    \"loss_func\": \"categorical_crossentropy\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "\n",
    "model = seq2seq_model(params)\n",
    "# batchã¯è«–æ–‡ã‚’å‚è€ƒã«è¨­å®šï¼æœ€é©åŒ–ã¯ã—ã¦ã„ãªã„\n",
    "# epochsã¯ã‚ã‚‹ç¨‹åº¦å›ã•ãªã„ã¨æœ€é©è§£ã«åˆ°é”ã—ãªã„ï¼\n",
    "batch_size = 128\n",
    "epochs = 90\n",
    "history = model.fit(onehot_enc_in, onehot_dec_out, \n",
    "                    batch_size=batch_size, epochs=epochs, validation_split=0.1, \n",
    "                    shuffle=\"batch\", # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å†…ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨è«–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨å®šå¯¾ç§°ã®å…¥åŠ›ç³»åˆ—ã‚’ç”¨æ„\n",
    "example_testtext = \"ãŠã¯ãã‚ƒãƒ¼ã£ã¨ï¼\" \n",
    "wakati_testtext= get_wakati(example_testtext, reverse=True)\n",
    "# æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã€€tokenizerã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ãŸã¨ãã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "token_testinput = tokenizer.texts_to_sequences(np.array([wakati_testtext]))\n",
    "# ç³»åˆ—é•·ã‚’æƒãˆã‚‹\n",
    "token_testinput = sequence.pad_sequences(token_testinput, maxlen=seq_maxlen)\n",
    "# one-hotè¡¨ç¾ã«å¤‰æ›\n",
    "onehot_testinput = np_utils.to_categorical(token_testinput, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_test_output = model.predict(onehot_testinput, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨å®šã—ãŸone-hotãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "token_testoutput = [np.argmax(seq_val) for seq_val in onehot_test_output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "reverse_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_testoutput = [val for val in token_testoutput if val > 0]\n",
    "pred_seq = [reverse_word_map[key_val] for key_val in token_testoutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
