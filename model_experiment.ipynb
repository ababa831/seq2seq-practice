{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各種Seq2Seqモデルを試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqの一番基本となる，4層のLSTMによるencoder-decoderモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, CuDNNLSTM, TimeDistributed\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "def get_wakati(text, eos=True, reverse=False):\n",
    "    \"\"\"\n",
    "    text: 文書(str) \n",
    "    eos=Trueの場合は，end of sentence（テキストの終端）記号を付加する\n",
    "    reverse=Trueの場合は，文字の並びを反転させる．（seq2seqでは，入力系列は反転しなければならない！）\n",
    "    \"\"\"\n",
    "    wakati_list = t.tokenize(text, wakati=True)\n",
    "    if reverse == True:\n",
    "        wakati_list = list(reversed(wakati_list))\n",
    "    if eos == True:\n",
    "        wakati_list.append(\"<EOS>\")\n",
    "    return \" \".join(wakati_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(df, target_colname):\n",
    "    total_word_count = Counter()\n",
    "    for text in df[target_colname]: \n",
    "        total_word_count += Counter(text.split())\n",
    "    return len(total_word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trn_feats(df, inp_colname=\"wakati_input\", out_colname=\"wakati_output\"):\n",
    "    \"\"\"\n",
    "    分かち書き済みの文書(入出力系列)を，特徴エンジニアリングする．\n",
    "    \n",
    "    [arguments]\n",
    "    df: 入出力関係を表すPandas DataFrame\n",
    "    inp_colname: 入力（encoder）系列のカラム名\n",
    "    out_colname: 出力（decoder）系列のカラム名\n",
    "    \n",
    "    [return values]\n",
    "    token_input: 数値ベクトル化された入力系列\n",
    "    onehot_output: one_hotベクトル化された出力系列\n",
    "    NNモデルの出力層はデータ内の全単語に関するsoftmax値を求めるため，ラベルデータは予めonehot化する必要がある．\n",
    "    tokenizer: 数値ベクトルに変換する際のモデル\n",
    "    params: 入出力データに関するパラメータをdictとして纏めたもの\n",
    "    \"\"\"\n",
    "    # 学習データの語彙数を計算する\n",
    "    num_words = words_count(df, inp_colname) + words_count(df, out_colname)\n",
    "    \n",
    "    # テキストを数値ベクトル表現に変換する\n",
    "    # 変換器の学習\n",
    "    tokenizer = text.Tokenizer(num_words=num_words)\n",
    "    inout_text_list = df[inp_colname].tolist() + df[out_colname].tolist()\n",
    "    tokenizer.fit_on_texts(inout_text_list)\n",
    "    # 最大の系列長を計算\n",
    "    inout_text_len = [len(text) for text in inout_text_list]\n",
    "    seq_maxlen = np.max(inout_text_len)\n",
    "    # 数値ベクトルに変換\n",
    "    token_input = tokenizer.texts_to_sequences(df[inp_colname].values)\n",
    "    token_output = tokenizer.texts_to_sequences(df[out_colname].values)\n",
    "    # 系列長を揃える\n",
    "    token_input = sequence.pad_sequences(token_input, maxlen=seq_maxlen)\n",
    "    token_output = sequence.pad_sequences(token_input, maxlen=seq_maxlen)\n",
    "    # one-hot形式に変換\n",
    "    onehot_output = np_utils.to_categorical(token_output, num_classes=num_words)\n",
    "    \n",
    "    # データに関する諸パラメータ\n",
    "    params = {\n",
    "        \"seq_maxlen\": seq_maxlen,\n",
    "        \"num_words\": num_words\n",
    "    }\n",
    "    \n",
    "    return token_input, onehot_output, tokenizer, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(new_params=None):\n",
    "    \"\"\"\n",
    "    最も基本的な4層LSTMによる，encoder-decoderモデル\n",
    "    Ref: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "    \n",
    "    hyper_params: カスタム設定したハイパーパラメータ\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"seq_maxlen\": 145,\n",
    "        \"num_words\": 1616,\n",
    "        \"vec_len\": 128,\n",
    "        \"learning_rate\": 0.002,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"loss_func\": \"categorical_crossentropy\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    }\n",
    "    if new_params != None:\n",
    "        params.update(new_params)\n",
    "        \n",
    "    inp = Input(shape=(params[\"seq_maxlen\"], ))\n",
    "    # end-to-end学習しやすいベクトルに写像するembedding層，その心はカーネルトリックに近い\n",
    "    emb = Embedding(params[\"num_words\"], params[\"vec_len\"])(inp) \n",
    "    dout= SpatialDropout1D(params[\"dropout_rate\"])(emb)\n",
    "    # 論文同様に，4層LSTMを指定．return_sequences=Trueで時系列分のLSTM演算を実行する．\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(dout)\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(lstmed)\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(lstmed)\n",
    "    lstmed = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(lstmed)\n",
    "    out = TimeDistributed(Dense(params[\"num_words\"], activation=\"softmax\"))(lstmed)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss=params[\"loss_func\"], \n",
    "                  optimizer=Adam(lr=params[\"learning_rate\"]),\n",
    "                  metrics=params[\"metrics\"]) # BLEUスコアで評価したいが，Kerasでは難しいので取り敢えず正解率で設定\n",
    "    model.summary() # モデルの構成を表示\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ねこますかわいいいいい</td>\n",
       "      <td>ありがと♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>のら！ちゃん！べりべりきゅーと！</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🔥猫松🔥</td>\n",
       "      <td>猫松さんが燃やされていますね</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction\n",
       "0          おはきゃーっと！         neglect\n",
       "1       ねこますかわいいいいい           ありがと♡\n",
       "2  のら！ちゃん！べりべりきゅーと！         neglect\n",
       "3          おはきゃーっと！         neglect\n",
       "4              🔥猫松🔥  猫松さんが燃やされていますね"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データは各自用意する．\n",
    "train_df = pd.read_csv(\"seq2seq_sample.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "      <th>wakati_input</th>\n",
       "      <th>wakati_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "      <td>！ っと きゃー は お &lt;EOS&gt;</td>\n",
       "      <td>neglect &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ねこますかわいいいいい</td>\n",
       "      <td>ありがと♡</td>\n",
       "      <td>いい いい わい か ます ねこ &lt;EOS&gt;</td>\n",
       "      <td>ありがと ♡ &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>のら！ちゃん！べりべりきゅーと！</td>\n",
       "      <td>neglect</td>\n",
       "      <td>！ と ー ゅ き べり べり ！ ちゃん ！ のら &lt;EOS&gt;</td>\n",
       "      <td>neglect &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "      <td>！ っと きゃー は お &lt;EOS&gt;</td>\n",
       "      <td>neglect &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🔥猫松🔥</td>\n",
       "      <td>猫松さんが燃やされていますね</td>\n",
       "      <td>🔥 松 猫 🔥 &lt;EOS&gt;</td>\n",
       "      <td>猫 松 さん が 燃やさ れ て い ます ね &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction                      wakati_input  \\\n",
       "0          おはきゃーっと！         neglect                ！ っと きゃー は お <EOS>   \n",
       "1       ねこますかわいいいいい           ありがと♡            いい いい わい か ます ねこ <EOS>   \n",
       "2  のら！ちゃん！べりべりきゅーと！         neglect  ！ と ー ゅ き べり べり ！ ちゃん ！ のら <EOS>   \n",
       "3          おはきゃーっと！         neglect                ！ っと きゃー は お <EOS>   \n",
       "4              🔥猫松🔥  猫松さんが燃やされていますね                     🔥 松 猫 🔥 <EOS>   \n",
       "\n",
       "                   wakati_output  \n",
       "0                  neglect <EOS>  \n",
       "1                   ありがと ♡ <EOS>  \n",
       "2                  neglect <EOS>  \n",
       "3                  neglect <EOS>  \n",
       "4  猫 松 さん が 燃やさ れ て い ます ね <EOS>  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"wakati_input\"] = train_df[\"__text\"].apply(lambda x: get_wakati(x, reverse=True))\n",
    "train_df[\"wakati_output\"] = train_df[\"reaction\"].apply(lambda x: get_wakati(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの語彙数を計算する\n",
    "nword_in = words_count(train_df, \"wakati_input\")\n",
    "nword_out = words_count(train_df, \"wakati_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを数値ベクトル表現に変換する\n",
    "# 1. 変換器の学習\n",
    "num_words = nword_in + nword_out\n",
    "tokenizer = text.Tokenizer(num_words=num_words)\n",
    "inout_text_list = train_df[\"wakati_input\"].tolist() + train_df[\"wakati_output\"].tolist()\n",
    "tokenizer.fit_on_texts(inout_text_list)\n",
    "# 2. 最大の系列長を計算\n",
    "inout_text_len = [len(text) for text in inout_text_list]\n",
    "seq_maxlen = np.max(inout_text_len)\n",
    "# 3. 数値ベクトルに変換\n",
    "token_input = tokenizer.texts_to_sequences(train_df[\"wakati_input\"].values)\n",
    "token_output = tokenizer.texts_to_sequences(train_df[\"wakati_output\"].values)\n",
    "# 系列長を揃える\n",
    "token_input = sequence.pad_sequences(token_input, maxlen=seq_maxlen)\n",
    "token_output = sequence.pad_sequences(token_output, maxlen=seq_maxlen)\n",
    "# ラベルデータはsoftmaxで推定するので，予めone-hot形式に変換しておく必要がある．\n",
    "onehot_output = np_utils.to_categorical(token_output, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数値ベクトルに変換した特徴量の例\n",
    "token_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0は文字列なし，1はEOS．それ以外は，単語が数値に変換されている\n",
    "逆変換する場合は，1以上の要素を抽出して，\n",
    "\n",
    "```\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "```\n",
    "\n",
    "を行えば良い？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数値データに戻す方法\n",
    "np.argmax(onehot_output[0,143,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "val_ratio = 0.1\n",
    "X_train, X_val, y_train, y_val = train_test_split(token_input, onehot_output, random_state=seed, test_size=val_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 145)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 145, 128)          206848    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 145, 128)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 145, 128)          132096    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 145, 1616)         208464    \n",
      "=================================================================\n",
      "Total params: 943,696\n",
      "Trainable params: 943,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3878 samples, validate on 431 samples\n",
      "Epoch 1/8\n",
      "3878/3878 [==============================] - 48s 12ms/step - loss: 2.0744 - acc: 0.9449 - val_loss: 0.2286 - val_acc: 0.9757\n",
      "Epoch 2/8\n",
      "3878/3878 [==============================] - 44s 11ms/step - loss: 0.1829 - acc: 0.9773 - val_loss: 0.1825 - val_acc: 0.9757\n",
      "Epoch 3/8\n",
      "3878/3878 [==============================] - 42s 11ms/step - loss: 0.1688 - acc: 0.9773 - val_loss: 0.1817 - val_acc: 0.9757\n",
      "Epoch 4/8\n",
      "3878/3878 [==============================] - 48s 12ms/step - loss: 0.1683 - acc: 0.9773 - val_loss: 0.1809 - val_acc: 0.9757\n",
      "Epoch 5/8\n",
      "3878/3878 [==============================] - 50s 13ms/step - loss: 0.1681 - acc: 0.9773 - val_loss: 0.1804 - val_acc: 0.9757\n",
      "Epoch 6/8\n",
      "3878/3878 [==============================] - 50s 13ms/step - loss: 0.1679 - acc: 0.9773 - val_loss: 0.1803 - val_acc: 0.9757\n",
      "Epoch 7/8\n",
      "3878/3878 [==============================] - 43s 11ms/step - loss: 0.1678 - acc: 0.9773 - val_loss: 0.1807 - val_acc: 0.9757\n",
      "Epoch 8/8\n",
      "3878/3878 [==============================] - 45s 12ms/step - loss: 0.1677 - acc: 0.9773 - val_loss: 0.1802 - val_acc: 0.9757\n"
     ]
    }
   ],
   "source": [
    "model = seq2seq_model()\n",
    "# batch. epochsは論文を参考に設定．最適化はしていない\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    # class_weight={0:0.01, 1:.99}, \n",
    "                    shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定対称の入力系列を用意\n",
    "example_testtext = \"🔥猫松🔥\" \n",
    "wakati_testtext= get_wakati(example_testtext, reverse=True)\n",
    "# 数値ベクトルに変換　tokenizerは学習データの特徴量を作成したときのモデルを使用\n",
    "token_testinput = tokenizer.texts_to_sequences(np.array([wakati_testtext]))\n",
    "# 系列長を揃える\n",
    "token_testinput = sequence.pad_sequences(token_testinput, maxlen=seq_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 326ms/step\n"
     ]
    }
   ],
   "source": [
    "onehot_test_output = model.predict(token_testinput, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定したone-hotデータを数値ベクトルに変換する\n",
    "token_testoutput = [np.argmax(seq_val) for seq_val in onehot_test_output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_testoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推定したデータがすべて0になってしまう．考えられる原因：\n",
    "\n",
    "- pad_sequenceで0で穴埋めしてしまったことにより，全ての値が0に過学習してしまった？\n",
    "- 学習データ数が少なすぎる？\n",
    "\n",
    "しかし，pad_sequenceで穴埋めしなければ，Kerasの学習は不可能？\n",
    "\n",
    "解決案：\n",
    "- 論文のように，系列長によってバッチを分ける？\n",
    "- データ数を増やせば0に過学習しなくなる？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
