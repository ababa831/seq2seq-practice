{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各種Seq2Seqモデルを試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqの一番基本となる，LSTMによるencoder-decoderモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, CuDNNLSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "def get_wakati(text, reverse=False, offset=False):\n",
    "    \"\"\"\n",
    "    text: 文書(str) \n",
    "    reverse=Trueの場合は，文字の並びを反転させる．（seq2seqでは，入力系列は反転しなければならない！）\n",
    "    offset=Trueの場合は，タイムステップ文\n",
    "    \"\"\"\n",
    "    wakati_list = t.tokenize(text, wakati=True)\n",
    "    if reverse == True:\n",
    "        wakati_list = list(reversed(wakati_list))\n",
    "    return \" \".join(wakati_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(df, target_cols):\n",
    "    total_word_count = Counter()\n",
    "    for col in target_cols:\n",
    "        for text in df[col]: \n",
    "            total_word_count += Counter(text.split())\n",
    "    #print(total_word_count.keys())\n",
    "    return len(total_word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charlen_count(df, target_colname):\n",
    "    df[\"tmp_wlist\"] = df[target_colname].str.split()\n",
    "    df[target_colname+\"_charlen\"] = df[\"tmp_wlist\"].apply(lambda x: len(x))\n",
    "    df = df.drop([\"tmp_wlist\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(new_params=None):\n",
    "    \"\"\"\n",
    "    最も基本的な4層LSTMによる，encoder-decoderモデル\n",
    "    Ref: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "    \n",
    "    hyper_params: カスタム設定したハイパーパラメータ\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"seq_maxlen\": 139,\n",
    "        \"num_words\": 1592,\n",
    "        \"vec_len\": 128,\n",
    "        \"learning_rate\": 0.002,\n",
    "        #\"dropout_rate\": 0.5,\n",
    "        \"loss_func\": \"categorical_crossentropy\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    }\n",
    "    if new_params != None:\n",
    "        params.update(new_params)\n",
    "        \n",
    "    enc_inp = Input(shape=(params[\"seq_maxlen\"], params[\"num_words\"]))\n",
    "    # decoder初期化用の内部状態を得る\n",
    "    enc_out, state_h, state_c = CuDNNLSTM(params[\"vec_len\"], return_sequences=True, return_state=True)(enc_inp)\n",
    "    enc_states = [state_h, state_c]\n",
    "    \n",
    "    dec_out = CuDNNLSTM(params[\"vec_len\"], return_sequences=True)(enc_out, initial_state=enc_states)\n",
    "    seq_out = Dense(params[\"num_words\"], activation=\"softmax\")(dec_out)\n",
    "    \n",
    "    model = Model(inputs=enc_inp, outputs=seq_out)\n",
    "    model.compile(loss=params[\"loss_func\"], \n",
    "                  optimizer=Adam(lr=params[\"learning_rate\"]),\n",
    "                  metrics=params[\"metrics\"]) # BLEUスコアで評価したいが，Kerasでは難しいので取り敢えず正解率で設定\n",
    "    model.summary() # モデルの構成を表示\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ねこますかわいいいいい</td>\n",
       "      <td>ありがと♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>のら！ちゃん！べりべりきゅーと！</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🔥猫松🔥</td>\n",
       "      <td>猫松さんが燃やされていますね</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction\n",
       "0          おはきゃーっと！         neglect\n",
       "1       ねこますかわいいいいい           ありがと♡\n",
       "2  のら！ちゃん！べりべりきゅーと！         neglect\n",
       "3          おはきゃーっと！         neglect\n",
       "4              🔥猫松🔥  猫松さんが燃やされていますね"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データは各自用意する．\n",
    "train_df = pd.read_csv(\"seq2seq_sample.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__text</th>\n",
       "      <th>reaction</th>\n",
       "      <th>wakati_enc_in</th>\n",
       "      <th>wakati_dec_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "      <td>！ っと きゃー は お</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ねこますかわいいいいい</td>\n",
       "      <td>ありがと♡</td>\n",
       "      <td>いい いい わい か ます ねこ</td>\n",
       "      <td>ありがと ♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>のら！ちゃん！べりべりきゅーと！</td>\n",
       "      <td>neglect</td>\n",
       "      <td>！ と ー ゅ き べり べり ！ ちゃん ！ のら</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>おはきゃーっと！</td>\n",
       "      <td>neglect</td>\n",
       "      <td>！ っと きゃー は お</td>\n",
       "      <td>neglect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>🔥猫松🔥</td>\n",
       "      <td>猫松さんが燃やされていますね</td>\n",
       "      <td>🔥 松 猫 🔥</td>\n",
       "      <td>猫 松 さん が 燃やさ れ て い ます ね</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             __text        reaction               wakati_enc_in  \\\n",
       "0          おはきゃーっと！         neglect                ！ っと きゃー は お   \n",
       "1       ねこますかわいいいいい           ありがと♡            いい いい わい か ます ねこ   \n",
       "2  のら！ちゃん！べりべりきゅーと！         neglect  ！ と ー ゅ き べり べり ！ ちゃん ！ のら   \n",
       "3          おはきゃーっと！         neglect                ！ っと きゃー は お   \n",
       "4              🔥猫松🔥  猫松さんが燃やされていますね                     🔥 松 猫 🔥   \n",
       "\n",
       "            wakati_dec_out  \n",
       "0                  neglect  \n",
       "1                   ありがと ♡  \n",
       "2                  neglect  \n",
       "3                  neglect  \n",
       "4  猫 松 さん が 燃やさ れ て い ます ね  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"wakati_enc_in\"] = train_df[\"__text\"].apply(lambda x: get_wakati(x, reverse=True))\n",
    "train_df[\"wakati_dec_out\"] = train_df[\"reaction\"].apply(lambda x: get_wakati(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データの語彙数を計算する\n",
    "target_cols = [\"wakati_enc_in\", \"wakati_dec_out\"]\n",
    "num_words = words_count(train_df, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_cols:\n",
    "    train_df = charlen_count(train_df, target_colname=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系列長順に並べ替える，ミニバッチ学習で系列長の差を小さくするため．\n",
    "train_df = train_df.sort_values(by=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを数値ベクトル表現に変換する\n",
    "# 1. 変換器の学習\n",
    "tokenizer = text.Tokenizer(num_words=num_words)\n",
    "inout_text_list = []\n",
    "for col in target_cols:\n",
    "    inout_text_list += train_df[col].tolist()\n",
    "tokenizer.fit_on_texts(inout_text_list)\n",
    "# 2. 最大の系列長を計算\n",
    "inout_text_len = [len(text) for text in inout_text_list]\n",
    "seq_maxlen = np.max(inout_text_len)\n",
    "# 3. 数値ベクトルに変換\n",
    "token_enc_in = tokenizer.texts_to_sequences(train_df[\"wakati_enc_in\"].values)\n",
    "token_dec_out = tokenizer.texts_to_sequences(train_df[\"wakati_dec_out\"].values)\n",
    "# 4. 系列長を揃える\n",
    "token_enc_in = sequence.pad_sequences(token_enc_in, maxlen=seq_maxlen)\n",
    "token_dec_out = sequence.pad_sequences(token_dec_out, maxlen=seq_maxlen)\n",
    "# 5. one-hot形式に変換\n",
    "onehot_enc_in = np_utils.to_categorical(token_enc_in, num_classes=num_words) \n",
    "onehot_dec_out = np_utils.to_categorical(token_dec_out, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数値ベクトルに変換した特徴量の例\n",
    "token_dec_out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0は文字列なし，1はEOS．それ以外は，単語が数値に変換されている\n",
    "逆変換する場合は，1以上の要素を抽出して，\n",
    "\n",
    "```\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "```\n",
    "\n",
    "で逆変換の辞書を作成して，系列を求めればよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_dec_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数値データに戻す方法\n",
    "np.argmax(onehot_dec_out[0,138,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 139, 1590)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 139, 128), ( 880640      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 139, 128)     132096      cu_dnnlstm_1[0][0]               \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 139, 1590)    205110      cu_dnnlstm_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,217,846\n",
      "Trainable params: 1,217,846\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3878 samples, validate on 431 samples\n",
      "Epoch 1/90\n",
      "3878/3878 [==============================] - 29s 7ms/step - loss: 2.1796 - acc: 0.9526 - val_loss: 0.8353 - val_acc: 0.9361\n",
      "Epoch 2/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0970 - acc: 0.9886 - val_loss: 0.8993 - val_acc: 0.9361\n",
      "Epoch 3/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0892 - acc: 0.9886 - val_loss: 0.8836 - val_acc: 0.9361\n",
      "Epoch 4/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0885 - acc: 0.9886 - val_loss: 0.8696 - val_acc: 0.9361\n",
      "Epoch 5/90\n",
      "3878/3878 [==============================] - 22s 6ms/step - loss: 0.0889 - acc: 0.9886 - val_loss: 0.8630 - val_acc: 0.9361\n",
      "Epoch 6/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0877 - acc: 0.9886 - val_loss: 0.8513 - val_acc: 0.9361\n",
      "Epoch 7/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0876 - acc: 0.9886 - val_loss: 0.8358 - val_acc: 0.9361\n",
      "Epoch 8/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0877 - acc: 0.9886 - val_loss: 0.8488 - val_acc: 0.9361\n",
      "Epoch 9/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0865 - acc: 0.9886 - val_loss: 0.8326 - val_acc: 0.9361\n",
      "Epoch 10/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0877 - acc: 0.9886 - val_loss: 0.8232 - val_acc: 0.9361\n",
      "Epoch 11/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0875 - acc: 0.9886 - val_loss: 0.8244 - val_acc: 0.9361\n",
      "Epoch 12/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0853 - acc: 0.9886 - val_loss: 0.8178 - val_acc: 0.9361\n",
      "Epoch 13/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0848 - acc: 0.9886 - val_loss: 0.8134 - val_acc: 0.9361\n",
      "Epoch 14/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0851 - acc: 0.9886 - val_loss: 0.8119 - val_acc: 0.9361\n",
      "Epoch 15/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0837 - acc: 0.9886 - val_loss: 0.8181 - val_acc: 0.9361\n",
      "Epoch 16/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0819 - acc: 0.9886 - val_loss: 0.8066 - val_acc: 0.9361\n",
      "Epoch 17/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0820 - acc: 0.9886 - val_loss: 0.8084 - val_acc: 0.9361\n",
      "Epoch 18/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0810 - acc: 0.9886 - val_loss: 0.8105 - val_acc: 0.9361\n",
      "Epoch 19/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0807 - acc: 0.9886 - val_loss: 0.8096 - val_acc: 0.9361\n",
      "Epoch 20/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0811 - acc: 0.9886 - val_loss: 0.8050 - val_acc: 0.9361\n",
      "Epoch 21/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0796 - acc: 0.9886 - val_loss: 0.8081 - val_acc: 0.9361\n",
      "Epoch 22/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0792 - acc: 0.9886 - val_loss: 0.8120 - val_acc: 0.9361\n",
      "Epoch 23/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0789 - acc: 0.9886 - val_loss: 0.8202 - val_acc: 0.9361\n",
      "Epoch 24/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0790 - acc: 0.9886 - val_loss: 0.8060 - val_acc: 0.9361\n",
      "Epoch 25/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0767 - acc: 0.9886 - val_loss: 0.8042 - val_acc: 0.9361\n",
      "Epoch 26/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0770 - acc: 0.9886 - val_loss: 0.7940 - val_acc: 0.9361\n",
      "Epoch 27/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0783 - acc: 0.9886 - val_loss: 0.8118 - val_acc: 0.9361\n",
      "Epoch 28/90\n",
      "3878/3878 [==============================] - 22s 6ms/step - loss: 0.0783 - acc: 0.9886 - val_loss: 0.8074 - val_acc: 0.9361\n",
      "Epoch 29/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0758 - acc: 0.9886 - val_loss: 0.7940 - val_acc: 0.9361\n",
      "Epoch 30/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0742 - acc: 0.9886 - val_loss: 0.7926 - val_acc: 0.9361\n",
      "Epoch 31/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0745 - acc: 0.9886 - val_loss: 0.8029 - val_acc: 0.9361\n",
      "Epoch 32/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0725 - acc: 0.9886 - val_loss: 0.7998 - val_acc: 0.9361\n",
      "Epoch 33/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0710 - acc: 0.9886 - val_loss: 0.7874 - val_acc: 0.9361\n",
      "Epoch 34/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0688 - acc: 0.9886 - val_loss: 0.7984 - val_acc: 0.9361\n",
      "Epoch 35/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0672 - acc: 0.9886 - val_loss: 0.8135 - val_acc: 0.9361\n",
      "Epoch 36/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0661 - acc: 0.9886 - val_loss: 0.7821 - val_acc: 0.9361\n",
      "Epoch 37/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0653 - acc: 0.9886 - val_loss: 0.7924 - val_acc: 0.9361\n",
      "Epoch 38/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0631 - acc: 0.9886 - val_loss: 0.7889 - val_acc: 0.9361\n",
      "Epoch 39/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0608 - acc: 0.9886 - val_loss: 0.7776 - val_acc: 0.9361\n",
      "Epoch 40/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0600 - acc: 0.9886 - val_loss: 0.7833 - val_acc: 0.9361\n",
      "Epoch 41/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0600 - acc: 0.9886 - val_loss: 0.7845 - val_acc: 0.9361\n",
      "Epoch 42/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0592 - acc: 0.9886 - val_loss: 0.7912 - val_acc: 0.9361\n",
      "Epoch 43/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0577 - acc: 0.9886 - val_loss: 0.7691 - val_acc: 0.9361\n",
      "Epoch 44/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0562 - acc: 0.9886 - val_loss: 0.7743 - val_acc: 0.9361\n",
      "Epoch 45/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0549 - acc: 0.9886 - val_loss: 0.7768 - val_acc: 0.9361\n",
      "Epoch 46/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0536 - acc: 0.9886 - val_loss: 0.7619 - val_acc: 0.9361\n",
      "Epoch 47/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0530 - acc: 0.9886 - val_loss: 0.7806 - val_acc: 0.9361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0541 - acc: 0.9886 - val_loss: 0.7663 - val_acc: 0.9361\n",
      "Epoch 49/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0516 - acc: 0.9886 - val_loss: 0.7837 - val_acc: 0.9361\n",
      "Epoch 50/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0513 - acc: 0.9886 - val_loss: 0.7657 - val_acc: 0.9361\n",
      "Epoch 51/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0517 - acc: 0.9887 - val_loss: 0.7649 - val_acc: 0.9361\n",
      "Epoch 52/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0516 - acc: 0.9886 - val_loss: 0.7472 - val_acc: 0.9361\n",
      "Epoch 53/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0503 - acc: 0.9886 - val_loss: 0.7702 - val_acc: 0.9361\n",
      "Epoch 54/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0505 - acc: 0.9886 - val_loss: 0.7483 - val_acc: 0.9361\n",
      "Epoch 55/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0489 - acc: 0.9886 - val_loss: 0.7705 - val_acc: 0.9367\n",
      "Epoch 56/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0479 - acc: 0.9894 - val_loss: 0.7741 - val_acc: 0.9361\n",
      "Epoch 57/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0763 - acc: 0.9761 - val_loss: 0.8507 - val_acc: 0.9361\n",
      "Epoch 58/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.1033 - acc: 0.9886 - val_loss: 0.7896 - val_acc: 0.9361\n",
      "Epoch 59/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0861 - acc: 0.9886 - val_loss: 0.8118 - val_acc: 0.9361\n",
      "Epoch 60/90\n",
      "3878/3878 [==============================] - 23s 6ms/step - loss: 0.0741 - acc: 0.9886 - val_loss: 0.8063 - val_acc: 0.9361\n",
      "Epoch 61/90\n",
      "3878/3878 [==============================] - 31s 8ms/step - loss: 0.0672 - acc: 0.9886 - val_loss: 0.7930 - val_acc: 0.9361\n",
      "Epoch 62/90\n",
      "3878/3878 [==============================] - 23s 6ms/step - loss: 0.0546 - acc: 0.9886 - val_loss: 0.7709 - val_acc: 0.9361\n",
      "Epoch 63/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0528 - acc: 0.9886 - val_loss: 0.7790 - val_acc: 0.9361\n",
      "Epoch 64/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0519 - acc: 0.9886 - val_loss: 0.7708 - val_acc: 0.9361\n",
      "Epoch 65/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0515 - acc: 0.9886 - val_loss: 0.7696 - val_acc: 0.9361\n",
      "Epoch 66/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0513 - acc: 0.9886 - val_loss: 0.7648 - val_acc: 0.9361\n",
      "Epoch 67/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0509 - acc: 0.9894 - val_loss: 0.7755 - val_acc: 0.9361\n",
      "Epoch 68/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0494 - acc: 0.9886 - val_loss: 0.7760 - val_acc: 0.9361\n",
      "Epoch 69/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0486 - acc: 0.9887 - val_loss: 0.7820 - val_acc: 0.9363\n",
      "Epoch 70/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0476 - acc: 0.9905 - val_loss: 0.7629 - val_acc: 0.9368\n",
      "Epoch 71/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0473 - acc: 0.9916 - val_loss: 0.7428 - val_acc: 0.9368\n",
      "Epoch 72/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0463 - acc: 0.9897 - val_loss: 0.7504 - val_acc: 0.9368\n",
      "Epoch 73/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0450 - acc: 0.9914 - val_loss: 0.7507 - val_acc: 0.9368\n",
      "Epoch 74/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0455 - acc: 0.9909 - val_loss: 0.7366 - val_acc: 0.9368\n",
      "Epoch 75/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0436 - acc: 0.9915 - val_loss: 0.7706 - val_acc: 0.9368\n",
      "Epoch 76/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0441 - acc: 0.9914 - val_loss: 0.7512 - val_acc: 0.9368\n",
      "Epoch 77/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0448 - acc: 0.9915 - val_loss: 0.7653 - val_acc: 0.9368\n",
      "Epoch 78/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0439 - acc: 0.9914 - val_loss: 0.7416 - val_acc: 0.9369\n",
      "Epoch 79/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0425 - acc: 0.9916 - val_loss: 0.7558 - val_acc: 0.9368\n",
      "Epoch 80/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0430 - acc: 0.9918 - val_loss: 0.7333 - val_acc: 0.9368\n",
      "Epoch 81/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0424 - acc: 0.9920 - val_loss: 0.7475 - val_acc: 0.9368\n",
      "Epoch 82/90\n",
      "3878/3878 [==============================] - 21s 6ms/step - loss: 0.0425 - acc: 0.9917 - val_loss: 0.7489 - val_acc: 0.9368\n",
      "Epoch 83/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0412 - acc: 0.9924 - val_loss: 0.7375 - val_acc: 0.9368\n",
      "Epoch 84/90\n",
      "3878/3878 [==============================] - 21s 5ms/step - loss: 0.0414 - acc: 0.9916 - val_loss: 0.7117 - val_acc: 0.9361\n",
      "Epoch 85/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0401 - acc: 0.9915 - val_loss: 0.7252 - val_acc: 0.9368\n",
      "Epoch 86/90\n",
      "3878/3878 [==============================] - 22s 6ms/step - loss: 0.0390 - acc: 0.9927 - val_loss: 0.7481 - val_acc: 0.9368\n",
      "Epoch 87/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0384 - acc: 0.9925 - val_loss: 0.7517 - val_acc: 0.9368\n",
      "Epoch 88/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0399 - acc: 0.9923 - val_loss: 0.7209 - val_acc: 0.9368\n",
      "Epoch 89/90\n",
      "3878/3878 [==============================] - 20s 5ms/step - loss: 0.0370 - acc: 0.9928 - val_loss: 0.7063 - val_acc: 0.9368\n",
      "Epoch 90/90\n",
      "1408/3878 [=========>....................] - ETA: 11s - loss: 0.0438 - acc: 0.9912"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"seq_maxlen\": seq_maxlen,\n",
    "    \"num_words\": num_words,\n",
    "    \"vec_len\": 128,\n",
    "    \"learning_rate\": 0.002,\n",
    "    #\"dropout_rate\": 0.5,\n",
    "    \"loss_func\": \"categorical_crossentropy\",\n",
    "    \"metrics\": [\"accuracy\"]\n",
    "}\n",
    "\n",
    "model = seq2seq_model(params)\n",
    "# batchは論文を参考に設定．最適化はしていない\n",
    "# epochsはある程度回さないと最適解に到達しない！\n",
    "batch_size = 128\n",
    "epochs = 90\n",
    "history = model.fit(onehot_enc_in, onehot_dec_out, \n",
    "                    batch_size=batch_size, epochs=epochs, validation_split=0.1, \n",
    "                    shuffle=\"batch\", # バッチデータ内でシャッフルする \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定対称の入力系列を用意\n",
    "example_testtext = \"おはきゃーっと！\" \n",
    "wakati_testtext= get_wakati(example_testtext, reverse=True)\n",
    "# 数値ベクトルに変換　tokenizerは学習データの特徴量を作成したときのモデルを使用\n",
    "token_testinput = tokenizer.texts_to_sequences(np.array([wakati_testtext]))\n",
    "# 系列長を揃える\n",
    "token_testinput = sequence.pad_sequences(token_testinput, maxlen=seq_maxlen)\n",
    "# one-hot表現に変換\n",
    "onehot_testinput = np_utils.to_categorical(token_testinput, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_test_output = model.predict(onehot_testinput, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定したone-hotデータを数値ベクトルに変換する\n",
    "token_testoutput = [np.argmax(seq_val) for seq_val in onehot_test_output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "reverse_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_testoutput = [val for val in token_testoutput if val > 0]\n",
    "pred_seq = [reverse_word_map[key_val] for key_val in token_testoutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
